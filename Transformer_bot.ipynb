{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fFCsiqNH0Hx01yUqOhClVMa5l2plBImk",
      "authorship_tag": "ABX9TyPu9x5Av2PEZalVN1XtlBFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christopherorea/transformers/blob/master/Transformer_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJTComIBL1",
        "outputId": "fb8e2dd4-2131-457c-977b-a0e7a79e2ab1"
      },
      "source": [
        "!pip install keras_transformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_transformer\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (2.4.3)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (1.4.1)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras_transformer) (1.15.0)\n",
            "Building wheels for collected packages: keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12944 sha256=6be363915ba2d7adfd0508d5d0e512e78038e19cfb21e7e9b4f01c712d9789bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=1f7481a384ddbd2b5dff702414a0028c84ae2420c12a1d186e9cf4dc1b7b3491\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=5b70cd95e49a4a9573c57f863a4e7f41d78ac1c1c269b3bef2d704e5eeac9e47\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5267 sha256=df730dca295dc51d244645032e058f1c9eaadd36dc9ebb4b3c0803950a031c77\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5625 sha256=83e137ce00db27f239ebb73564768b54f71fdd257e15a73373ece7d807b812b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=d19933015e87d40cdf510357e24ff2c1c74fd5d9510b7df55de116182b56fc78\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=92980475ee2ff1d67c560984f7e59f404ed734721ce4bfae0f38a054aac5b491\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer\n",
            "Successfully installed keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UUbbbSzH-h_"
      },
      "source": [
        "import numpy as np \n",
        "from keras_transformer import get_model, decode\n",
        "from pickle import load\n",
        "import h5py\n",
        "np.random.seed(89)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uHLRr8dWaD1"
      },
      "source": [
        "import json\n",
        "false = False\n",
        "true = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGWxXZmVYOn8"
      },
      "source": [
        "persona1= data['participants'][0]\n",
        "persona2 = data['participants'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_SY052bYJuv"
      },
      "source": [
        "for message in data['messages']:\n",
        "  if 'content' in message and ():\n",
        "    print(message['content'].encode('latin1').decode('utf8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9e255graUIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510f4b36-32f0-4fce-fa82-47ccbbcefc5b"
      },
      "source": [
        "!git clone 'https://github.com/nshepperd/gpt-2.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 375, done.\u001b[K\n",
            "remote: Total 375 (delta 0), reused 0 (delta 0), pack-reused 375\u001b[K\n",
            "Receiving objects: 100% (375/375), 4.42 MiB | 22.99 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFA5sG8DMw7J"
      },
      "source": [
        "!mkdir models\n",
        "!mkdir models/124M"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj-wRAJGCwUj",
        "outputId": "7184254b-e7a5-484e-8a1d-9894736f2b12"
      },
      "source": [
        "model = '124M'\n",
        "\n",
        "subdir = os.path.join('models', model)\n",
        "\n",
        "for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
        "\n",
        "    r = requests.get(\"https://openaipublic.blob.core.windows.net/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
        "\n",
        "    with open(os.path.join(subdir, filename), 'wb') as f:\n",
        "        file_size = int(r.headers[\"content-length\"])\n",
        "        chunk_size = 1000\n",
        "        with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
        "            # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
        "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                f.write(chunk)\n",
        "                pbar.update(chunk_size)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 353kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 4.31Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 255kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:12, 40.9Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 2.09Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 2.34Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 2.79Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDQnUrx8DHDV"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl-6I46_DzNT"
      },
      "source": [
        "df = pd.read_csv('train.csv', index_col=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcV7R3O6D21q"
      },
      "source": [
        "corpus = []\n",
        "\n",
        "for text in df.apply(lambda x: x).values:\n",
        "    corpus.append(f'{text[0]}. {text[1]}')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSSDvFe6EIys"
      },
      "source": [
        "df.to_csv(r'corpus.txt', header=None, index=None, sep=' ', mode='a')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ1D6kPiEb1S",
        "outputId": "d378b2d3-bb67-4bcf-b3a8-9882aa2d161c"
      },
      "source": [
        "!PYTHONPATH=gpt-2/src ./gpt-2/encode.py corpus.txt corpus.npz"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-01 05:15:01.623765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Reading files\n",
            "100% 1/1 [00:03<00:00,  3.80s/it]\n",
            "Writing corpus.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N_j7N6yNsuS",
        "outputId": "56a81589-11ae-412d-ee7d-8fb8972169eb"
      },
      "source": [
        "!pip3 install -r ./gpt-2/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from -r ./gpt-2/requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.7/dist-packages (from -r ./gpt-2/requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.7/dist-packages (from -r ./gpt-2/requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.7/dist-packages (from -r ./gpt-2/requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.7/dist-packages (from -r ./gpt-2/requirements.txt (line 5)) (1.5)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r ./gpt-2/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r ./gpt-2/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r ./gpt-2/requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r ./gpt-2/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r ./gpt-2/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r ./gpt-2/requirements.txt (line 3)) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfZB5_OuRI0H",
        "outputId": "aa52b011-8749-456b-decc-5d2eaaa28235"
      },
      "source": [
        "!pip install tensorflow==1.15.3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.3\n",
            "  Using cached https://files.pythonhosted.org/packages/c7/d3/4501a075355733b90bc384c900ae4f84df702c340f4034741647c63a5acd/tensorflow-1.15.3-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (0.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (1.19.5)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (0.2.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Using cached https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3) (1.32.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.3) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (3.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (3.7.4.3)\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.13.1\n",
            "    Uninstalling tensorboard-1.13.1:\n",
            "      Successfully uninstalled tensorboard-1.13.1\n",
            "  Found existing installation: tensorflow-estimator 1.13.0\n",
            "    Uninstalling tensorflow-estimator-1.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.13.0\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed tensorboard-1.15.0 tensorflow-1.15.3 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHfDW6i4NgvA",
        "outputId": "d419f3e3-5430-48b8-c2c9-f4c7e72ce40e"
      },
      "source": [
        "!PYTHONPATH=gpt-2/src ./gpt-2/train.py --model_name 124M --dataset corpus.npz \\\n",
        "    --batch_size 2 --save_every 10000 --sample_every 1000"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NpVTa9EdBrK",
        "outputId": "7b797ac0-e95d-43c2-9c99-28f990b4c35e"
      },
      "source": [
        "!python gpt-2/src/generate_unconditional_samples.py --top_k 3 --temperature 0.9 --nsamples 2 --seed 0 \\\n",
        "    --model_name 124M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/generate_unconditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-03-01 08:05:01.935659: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-03-01 08:05:01.938898: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-03-01 08:05:01.939081: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562071f832c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-03-01 08:05:01.939111: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From gpt-2/src/generate_unconditional_samples.py:54: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From gpt-2/src/generate_unconditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoutBvjC0Pt0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}